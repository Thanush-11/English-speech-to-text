{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jdy0nYOpSdcI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, random, math, time, tarfile, urllib.request\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import sentencepiece as spm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from jiwer import wer, cer\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G5Ol_qxJSfpg"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./datasets\"\n",
    "TOKENIZER_DIR = \"./tokenizers\"\n",
    "CHECKPOINT_DIR = \"./checkpoints_en\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# LibriSpeech splits (recommended for real evaluation)\n",
    "DOWNLOAD_SPLITS = {\n",
    "    \"train-clean-100\": True,\n",
    "    \"dev-clean\": True,\n",
    "    \"test-clean\": True,\n",
    "}\n",
    "\n",
    "VOCAB_SIZE = 8000\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "LR = 3e-4\n",
    "LOG_EVERY = 50\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# Data constraints\n",
    "MAX_AUDIO_SEC = 20.0      # we will SKIP (not truncate) audio longer than this\n",
    "MAX_TOK_LEN = 250         # optionally skip extreme transcripts\n",
    "\n",
    "# Training tricks (optional)\n",
    "USE_LABEL_SMOOTHING = True\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "USE_SPEC_AUG = False      # start False; enable later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eWuIaPTdSh3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-clean-100 ...\n",
      "Extracting train-clean-100 ...\n",
      "train-clean-100: ready at ./datasets\\LibriSpeech\\LibriSpeech\\train-clean-100\n",
      "Downloading dev-clean ...\n",
      "Extracting dev-clean ...\n",
      "dev-clean: ready at ./datasets\\LibriSpeech\\LibriSpeech\\dev-clean\n",
      "Downloading test-clean ...\n",
      "Extracting test-clean ...\n",
      "test-clean: ready at ./datasets\\LibriSpeech\\LibriSpeech\\test-clean\n",
      "Split paths: {'train-clean-100': './datasets\\\\LibriSpeech\\\\LibriSpeech\\\\train-clean-100', 'dev-clean': './datasets\\\\LibriSpeech\\\\LibriSpeech\\\\dev-clean', 'test-clean': './datasets\\\\LibriSpeech\\\\LibriSpeech\\\\test-clean'}\n"
     ]
    }
   ],
   "source": [
    "OPENSLR_BASE = \"https://www.openslr.org/resources/12\"\n",
    "SPLIT_URLS = {\n",
    "    \"train-clean-100\": f\"{OPENSLR_BASE}/train-clean-100.tar.gz\",\n",
    "    \"dev-clean\":       f\"{OPENSLR_BASE}/dev-clean.tar.gz\",\n",
    "    \"test-clean\":      f\"{OPENSLR_BASE}/test-clean.tar.gz\",\n",
    "}\n",
    "\n",
    "def download_and_extract_librispeech(split_name: str):\n",
    "    url = SPLIT_URLS[split_name]\n",
    "    tar_path = os.path.join(DATA_DIR, f\"{split_name}.tar.gz\")\n",
    "    extract_root = os.path.join(DATA_DIR, \"LibriSpeech\")\n",
    "\n",
    "    os.makedirs(extract_root, exist_ok=True)\n",
    "\n",
    "    # Check if already extracted\n",
    "    expected = os.path.join(extract_root, split_name)\n",
    "    if os.path.isdir(expected) and len(glob.glob(os.path.join(expected, \"**\", \"*.flac\"), recursive=True)) > 0:\n",
    "        print(f\"{split_name}: already present.\")\n",
    "        return expected\n",
    "\n",
    "    # Download tar if missing\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Downloading {split_name} ...\")\n",
    "        urllib.request.urlretrieve(url, tar_path)\n",
    "\n",
    "    # Extract\n",
    "    print(f\"Extracting {split_name} ...\")\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(extract_root)\n",
    "\n",
    "    # LibriSpeech extracts to DATA_DIR/LibriSpeech/LibriSpeech/<split>\n",
    "    # Some people end up with double folder; handle both.\n",
    "    candidates = [\n",
    "        os.path.join(extract_root, \"LibriSpeech\", split_name),\n",
    "        os.path.join(extract_root, split_name),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if os.path.isdir(c) and len(glob.glob(os.path.join(c, \"**\", \"*.flac\"), recursive=True)) > 0:\n",
    "            print(f\"{split_name}: ready at {c}\")\n",
    "            return c\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find extracted split folder for {split_name}.\")\n",
    "\n",
    "paths = {}\n",
    "for split, do in DOWNLOAD_SPLITS.items():\n",
    "    if do:\n",
    "        paths[split] = download_and_extract_librispeech(split)\n",
    "\n",
    "print(\"Split paths:\", paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vnsTT2ZLSlYa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 28539\n",
      "Val pairs: 2703\n",
      "Test pairs: 2620\n",
      "Sample: ('./datasets\\\\LibriSpeech\\\\LibriSpeech\\\\train-clean-100\\\\103\\\\1240\\\\103-1240-0000.flac', 'CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK')\n"
     ]
    }
   ],
   "source": [
    "def build_librispeech_pairs(split_root: str):\n",
    "    trans_map = {}\n",
    "    for trans_path in glob.glob(os.path.join(split_root, \"**\", \"*.trans.txt\"), recursive=True):\n",
    "        with open(trans_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                utt_id, text = line.split(\" \", 1)\n",
    "                trans_map[utt_id] = text\n",
    "\n",
    "    pairs = []\n",
    "    for flac in glob.glob(os.path.join(split_root, \"**\", \"*.flac\"), recursive=True):\n",
    "        utt_id = os.path.basename(flac).replace(\".flac\", \"\")\n",
    "        if utt_id in trans_map:\n",
    "            pairs.append((flac, trans_map[utt_id]))\n",
    "    return pairs\n",
    "\n",
    "train_pairs = build_librispeech_pairs(paths[\"train-clean-100\"])\n",
    "val_pairs   = build_librispeech_pairs(paths[\"dev-clean\"])\n",
    "test_pairs  = build_librispeech_pairs(paths[\"test-clean\"])\n",
    "\n",
    "print(\"Train pairs:\", len(train_pairs))\n",
    "print(\"Val pairs:\", len(val_pairs))\n",
    "print(\"Test pairs:\", len(test_pairs))\n",
    "print(\"Sample:\", train_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IXtu8HtFSnnp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece tokenizer ...\n",
      "Tokenizer trained: ./tokenizers\\en_sp.model\n",
      "Vocab: 8000 | PAD/UNK/BOS/EOS: 0 1 2 3\n",
      "Sanity: this is a test\n"
     ]
    }
   ],
   "source": [
    "SP_PREFIX = os.path.join(TOKENIZER_DIR, \"en_sp\")\n",
    "SP_MODEL  = SP_PREFIX + \".model\"\n",
    "SP_VOCAB  = SP_PREFIX + \".vocab\"\n",
    "\n",
    "# If you changed settings, DELETE old model+vocab before running this cell.\n",
    "if not os.path.exists(SP_MODEL):\n",
    "    print(\"Training SentencePiece tokenizer ...\")\n",
    "    txt_path = os.path.join(TOKENIZER_DIR, \"en_transcripts.txt\")\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, t in train_pairs:\n",
    "            f.write(t.strip().lower() + \"\\n\")\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=txt_path,\n",
    "        model_prefix=SP_PREFIX,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=1.0,\n",
    "        pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "        byte_fallback=True,   # ✅ important for robustness\n",
    "    )\n",
    "    print(\"Tokenizer trained:\", SP_MODEL)\n",
    "else:\n",
    "    print(\"Tokenizer exists:\", SP_MODEL)\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=SP_MODEL)\n",
    "PAD_ID = sp.pad_id()\n",
    "UNK_ID = sp.unk_id()\n",
    "BOS_ID = sp.bos_id()\n",
    "EOS_ID = sp.eos_id()\n",
    "\n",
    "print(\"Vocab:\", sp.get_piece_size(), \"| PAD/UNK/BOS/EOS:\", PAD_ID, UNK_ID, BOS_ID, EOS_ID)\n",
    "print(\"Sanity:\", sp.decode(sp.encode(\"this is a test\", out_type=int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nphr7oEcSp0A"
   },
   "outputs": [],
   "source": [
    "def load_audio_16k_mono(path: str):\n",
    "    wav, sr = torchaudio.load(path)  # (C, T)\n",
    "    if wav.size(0) > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    return wav.squeeze(0), 16000  # (T,), 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O-LIeWs2SuJV"
   },
   "outputs": [],
   "source": [
    "class LibriTokenDataset(Dataset):\n",
    "    def __init__(self, pairs, sp, max_audio_sec=20.0, max_tok_len=250):\n",
    "        self.pairs = pairs\n",
    "        self.sp = sp\n",
    "        self.max_audio_samples = int(max_audio_sec * 16000)\n",
    "        self.max_tok_len = max_tok_len\n",
    "\n",
    "        # ✅ build transform once (not inside __getitem__)\n",
    "        self.mel_fn = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, text = self.pairs[idx]\n",
    "        wav, _ = load_audio_16k_mono(path)\n",
    "\n",
    "        # ✅ skip (do not truncate) to avoid audio/text mismatch\n",
    "        if wav.numel() > self.max_audio_samples:\n",
    "            return self.__getitem__((idx + 1) % len(self.pairs))\n",
    "\n",
    "        text = text.strip().lower()\n",
    "        ids = self.sp.encode(text, out_type=int)\n",
    "        if len(ids) > self.max_tok_len:\n",
    "            return self.__getitem__((idx + 1) % len(self.pairs))\n",
    "\n",
    "        ids = [BOS_ID] + ids + [EOS_ID]\n",
    "        tok = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "        mel = self.mel_fn(wav.unsqueeze(0))              # (1, 80, frames)\n",
    "        feat = torch.log(mel + 1e-9).squeeze(0).T        # (T, 80)\n",
    "        return feat, tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fXa4C47zSzPp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders ready: 1784 169 164\n"
     ]
    }
   ],
   "source": [
    "def collate_pad(batch):\n",
    "    feats, toks = zip(*batch)\n",
    "    feat_lens = torch.tensor([f.size(0) for f in feats], dtype=torch.long)\n",
    "    tok_lens  = torch.tensor([t.size(0) for t in toks], dtype=torch.long)\n",
    "\n",
    "    maxT = int(feat_lens.max())\n",
    "    maxL = int(tok_lens.max())\n",
    "\n",
    "    feat_pad = torch.zeros(len(batch), maxT, feats[0].size(1), dtype=torch.float32)\n",
    "    tok_pad  = torch.full((len(batch), maxL), PAD_ID, dtype=torch.long)\n",
    "\n",
    "    for i, (f, t) in enumerate(zip(feats, toks)):\n",
    "        feat_pad[i, : f.size(0)] = f\n",
    "        tok_pad[i, : t.size(0)]  = t\n",
    "\n",
    "    return feat_pad, tok_pad, feat_lens, tok_lens\n",
    "\n",
    "train_ds = LibriTokenDataset(train_pairs, sp, max_audio_sec=MAX_AUDIO_SEC, max_tok_len=MAX_TOK_LEN)\n",
    "val_ds   = LibriTokenDataset(val_pairs,   sp, max_audio_sec=MAX_AUDIO_SEC, max_tok_len=MAX_TOK_LEN)\n",
    "test_ds  = LibriTokenDataset(test_pairs,  sp, max_audio_sec=MAX_AUDIO_SEC, max_tok_len=MAX_TOK_LEN)\n",
    "\n",
    "# ✅ num_workers=0 for Windows notebook stability\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_pad,\n",
    "                          num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_pad,\n",
    "                          num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_pad,\n",
    "                          num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "print(\"Loaders ready:\", len(train_loader), len(val_loader), len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZEeNcp7zS28n"
   },
   "outputs": [],
   "source": [
    "def make_key_padding_mask(lengths, max_len):\n",
    "    idx = torch.arange(max_len, device=lengths.device).unsqueeze(0)\n",
    "    return idx >= lengths.unsqueeze(1)   # (B, T) True where padded\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T]\n",
    "\n",
    "def make_key_padding_mask(lengths, max_len):\n",
    "    idx = torch.arange(max_len, device=lengths.device).unsqueeze(0)\n",
    "    return idx >= lengths.unsqueeze(1)   # bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X_FsPbtgS8Mj"
   },
   "outputs": [],
   "source": [
    "class ConvSubsample(nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer Conv2D subsampling: T -> ~T/4\n",
    "    Input:  (B, T, F)\n",
    "    Output: (B, T', d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feats=80, d_model=256, channels=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        f_out = (in_feats + 1) // 2\n",
    "        f_out = (f_out + 1) // 2\n",
    "        self.out = nn.Linear(channels * f_out, d_model)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x.unsqueeze(1)             # (B,1,T,F)\n",
    "        x = self.conv(x)               # (B,C,T',F')\n",
    "        B, C, T2, F2 = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(B, T2, C * F2)\n",
    "        x = self.out(x)\n",
    "\n",
    "        l1 = (lengths + 1) // 2\n",
    "        l2 = (l1 + 1) // 2\n",
    "        return x, l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CjmHrzM-S_Il"
   },
   "outputs": [],
   "source": [
    "class ASRTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        pad_id: int,\n",
    "        n_mels: int = 80,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 4,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.subsample = ConvSubsample(in_feats=n_mels, d_model=d_model, channels=32)\n",
    "        self.pos_enc   = PositionalEncoding(d_model)\n",
    "\n",
    "        self.tok_emb   = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_lengths, tgt_key_padding_mask=None):\n",
    "        # src: (B, Ts, 80) | tgt: (B, Lt)\n",
    "        src, src_lengths2 = self.subsample(src, src_lengths)     # (B, Ts', d)\n",
    "        src = src * math.sqrt(self.d_model)\n",
    "        src = self.pos_enc(src)\n",
    "        src = src.transpose(0, 1)                                # (Ts', B, d)\n",
    "\n",
    "        Ts2 = src.size(0)\n",
    "        src_pad_mask = make_key_padding_mask(src_lengths2, Ts2)  # (B, Ts')\n",
    "\n",
    "        tgt = self.tok_emb(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "        tgt = tgt.transpose(0, 1)                                # (Lt, B, d)\n",
    "\n",
    "        tgt_mask = subsequent_mask(tgt.size(0), tgt.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            src, tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_pad_mask,\n",
    "        )\n",
    "        out = out.transpose(0, 1)  # (B, Lt, d)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4k8YU6FYTCjl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thanu\\english_training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting fresh.\n",
      "Params (M): 12.186016\n"
     ]
    }
   ],
   "source": [
    "model = ASRTransformer(vocab_size=sp.get_piece_size(), pad_id=PAD_ID).to(device)\n",
    "\n",
    "if USE_LABEL_SMOOTHING:\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=LABEL_SMOOTHING)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.98), weight_decay=1e-2)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_cuda)\n",
    "\n",
    "LAST_CKPT = os.path.join(CHECKPOINT_DIR, \"asr_en_last.pt\")\n",
    "BEST_CKPT = os.path.join(CHECKPOINT_DIR, \"asr_en_best.pt\")\n",
    "\n",
    "@dataclass\n",
    "class TrainState:\n",
    "    epoch: int = 0\n",
    "    best_val: float = 1e9\n",
    "\n",
    "state = TrainState()\n",
    "\n",
    "if os.path.exists(LAST_CKPT):\n",
    "    ckpt = torch.load(LAST_CKPT, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optim\"])\n",
    "    state.epoch = ckpt.get(\"epoch\", 0)\n",
    "    state.best_val = ckpt.get(\"best_val\", 1e9)\n",
    "    print(f\"Resumed from epoch {state.epoch}, best_val={state.best_val:.4f}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "print(\"Params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QhRQ5iecTK7Q"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def run_eval_loss(loader):\n",
    "    model.eval()\n",
    "    total_loss, steps = 0.0, 0\n",
    "\n",
    "    for feats, toks, feat_lens, tok_lens in loader:\n",
    "        feats, toks = feats.to(device), toks.to(device)\n",
    "        feat_lens, tok_lens = feat_lens.to(device), tok_lens.to(device)\n",
    "\n",
    "        tgt_in  = toks[:, :-1]\n",
    "        tgt_out = toks[:, 1:]\n",
    "        tgt_pad_mask = make_key_padding_mask(tok_lens - 1, tgt_in.size(1))\n",
    "\n",
    "        logits = model(feats, tgt_in, src_lengths=feat_lens, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        steps += 1\n",
    "\n",
    "    return total_loss / max(1, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def subsequent_mask(size: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Generate a causal (look-ahead) mask for Transformer decoder.\n",
    "    Shape: (size, size)\n",
    "    \"\"\"\n",
    "    return torch.triu(\n",
    "        torch.ones(size, size, device=device, dtype=torch.bool),\n",
    "        diagonal=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "txyNj4_rTRKe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1/25 | batch 00001 | loss 9.1936\n",
      "E1/25 | batch 00050 | loss 6.8160\n",
      "E1/25 | batch 00100 | loss 6.7713\n",
      "E1/25 | batch 00150 | loss 6.6492\n",
      "E1/25 | batch 00200 | loss 6.6592\n",
      "E1/25 | batch 00250 | loss 6.7142\n",
      "E1/25 | batch 00300 | loss 6.6077\n",
      "E1/25 | batch 00350 | loss 6.3930\n",
      "E1/25 | batch 00400 | loss 6.4021\n",
      "E1/25 | batch 00450 | loss 6.3427\n",
      "E1/25 | batch 00500 | loss 6.4428\n",
      "E1/25 | batch 00550 | loss 6.3677\n",
      "E1/25 | batch 00600 | loss 6.3221\n",
      "E1/25 | batch 00650 | loss 6.0563\n",
      "E1/25 | batch 00700 | loss 6.1656\n",
      "E1/25 | batch 00750 | loss 6.0675\n",
      "E1/25 | batch 00800 | loss 6.0909\n",
      "E1/25 | batch 00850 | loss 6.1539\n",
      "E1/25 | batch 00900 | loss 6.3288\n",
      "E1/25 | batch 00950 | loss 6.1328\n",
      "E1/25 | batch 01000 | loss 6.0092\n",
      "E1/25 | batch 01050 | loss 6.1644\n",
      "E1/25 | batch 01100 | loss 6.0557\n",
      "E1/25 | batch 01150 | loss 6.0179\n",
      "E1/25 | batch 01200 | loss 6.0183\n",
      "E1/25 | batch 01250 | loss 6.2742\n",
      "E1/25 | batch 01300 | loss 5.9616\n",
      "E1/25 | batch 01350 | loss 6.2090\n",
      "E1/25 | batch 01400 | loss 6.0582\n",
      "E1/25 | batch 01450 | loss 6.0722\n",
      "E1/25 | batch 01500 | loss 6.0689\n",
      "E1/25 | batch 01550 | loss 6.1701\n",
      "E1/25 | batch 01600 | loss 6.0139\n",
      "E1/25 | batch 01650 | loss 5.9664\n",
      "E1/25 | batch 01700 | loss 6.0146\n",
      "E1/25 | batch 01750 | loss 5.9185\n",
      "Epoch 1/25 done in 325.8s | train=6.2723 | val=5.9383 ✅ best\n",
      "E2/25 | batch 00001 | loss 5.8388\n",
      "E2/25 | batch 00050 | loss 6.0424\n",
      "E2/25 | batch 00100 | loss 5.7984\n",
      "E2/25 | batch 00150 | loss 5.9580\n",
      "E2/25 | batch 00200 | loss 5.7923\n",
      "E2/25 | batch 00250 | loss 5.7248\n",
      "E2/25 | batch 00300 | loss 6.0392\n",
      "E2/25 | batch 00350 | loss 6.1061\n",
      "E2/25 | batch 00400 | loss 5.7785\n",
      "E2/25 | batch 00450 | loss 5.8154\n",
      "E2/25 | batch 00500 | loss 5.8577\n",
      "E2/25 | batch 00550 | loss 5.8700\n",
      "E2/25 | batch 00600 | loss 5.8350\n",
      "E2/25 | batch 00650 | loss 6.2484\n",
      "E2/25 | batch 00700 | loss 5.9827\n",
      "E2/25 | batch 00750 | loss 5.9544\n",
      "E2/25 | batch 00800 | loss 5.8654\n",
      "E2/25 | batch 00850 | loss 5.7395\n",
      "E2/25 | batch 00900 | loss 5.8626\n",
      "E2/25 | batch 00950 | loss 5.8210\n",
      "E2/25 | batch 01000 | loss 5.7810\n",
      "E2/25 | batch 01050 | loss 5.8645\n",
      "E2/25 | batch 01100 | loss 5.9334\n",
      "E2/25 | batch 01150 | loss 5.7585\n",
      "E2/25 | batch 01200 | loss 5.9441\n",
      "E2/25 | batch 01250 | loss 5.7123\n",
      "E2/25 | batch 01300 | loss 5.6897\n",
      "E2/25 | batch 01350 | loss 5.7820\n",
      "E2/25 | batch 01400 | loss 5.8404\n",
      "E2/25 | batch 01450 | loss 5.6641\n",
      "E2/25 | batch 01500 | loss 5.7897\n",
      "E2/25 | batch 01550 | loss 5.9555\n",
      "E2/25 | batch 01600 | loss 6.0516\n",
      "E2/25 | batch 01650 | loss 5.6938\n",
      "E2/25 | batch 01700 | loss 5.7437\n",
      "E2/25 | batch 01750 | loss 5.7944\n",
      "Epoch 2/25 done in 312.1s | train=5.8210 | val=5.7061 ✅ best\n",
      "E3/25 | batch 00001 | loss 5.4935\n",
      "E3/25 | batch 00050 | loss 5.3907\n",
      "E3/25 | batch 00100 | loss 5.6398\n",
      "E3/25 | batch 00150 | loss 5.5936\n",
      "E3/25 | batch 00200 | loss 5.8099\n",
      "E3/25 | batch 00250 | loss 5.5828\n",
      "E3/25 | batch 00300 | loss 5.6102\n",
      "E3/25 | batch 00350 | loss 5.6296\n",
      "E3/25 | batch 00400 | loss 5.7164\n",
      "E3/25 | batch 00450 | loss 5.4863\n",
      "E3/25 | batch 00500 | loss 5.7640\n",
      "E3/25 | batch 00550 | loss 5.7477\n",
      "E3/25 | batch 00600 | loss 5.3520\n",
      "E3/25 | batch 00650 | loss 5.5930\n",
      "E3/25 | batch 00700 | loss 5.6397\n",
      "E3/25 | batch 00750 | loss 5.6017\n",
      "E3/25 | batch 00800 | loss 5.6662\n",
      "E3/25 | batch 00850 | loss 5.5656\n",
      "E3/25 | batch 00900 | loss 5.6626\n",
      "E3/25 | batch 00950 | loss 5.6524\n",
      "E3/25 | batch 01000 | loss 5.6619\n",
      "E3/25 | batch 01050 | loss 5.6233\n",
      "E3/25 | batch 01100 | loss 5.6863\n",
      "E3/25 | batch 01150 | loss 5.6473\n",
      "E3/25 | batch 01200 | loss 5.4954\n",
      "E3/25 | batch 01250 | loss 5.8419\n",
      "E3/25 | batch 01300 | loss 5.6408\n",
      "E3/25 | batch 01350 | loss 5.6979\n",
      "E3/25 | batch 01400 | loss 5.5866\n",
      "E3/25 | batch 01450 | loss 5.6337\n",
      "E3/25 | batch 01500 | loss 5.7416\n",
      "E3/25 | batch 01550 | loss 5.5766\n",
      "E3/25 | batch 01600 | loss 5.5649\n",
      "E3/25 | batch 01650 | loss 5.6789\n",
      "E3/25 | batch 01700 | loss 5.5071\n",
      "E3/25 | batch 01750 | loss 5.6264\n",
      "Epoch 3/25 done in 309.2s | train=5.6289 | val=5.6134 ✅ best\n",
      "E4/25 | batch 00001 | loss 5.3270\n",
      "E4/25 | batch 00050 | loss 5.2982\n",
      "E4/25 | batch 00100 | loss 5.4483\n",
      "E4/25 | batch 00150 | loss 5.2281\n",
      "E4/25 | batch 00200 | loss 5.4254\n",
      "E4/25 | batch 00250 | loss 5.2619\n",
      "E4/25 | batch 00300 | loss 5.3231\n",
      "E4/25 | batch 00350 | loss 5.6054\n",
      "E4/25 | batch 00400 | loss 5.5557\n",
      "E4/25 | batch 00450 | loss 5.3609\n",
      "E4/25 | batch 00500 | loss 5.5098\n",
      "E4/25 | batch 00550 | loss 5.3912\n",
      "E4/25 | batch 00600 | loss 5.6115\n",
      "E4/25 | batch 00650 | loss 5.3244\n",
      "E4/25 | batch 00700 | loss 5.5367\n",
      "E4/25 | batch 00750 | loss 5.3917\n",
      "E4/25 | batch 00800 | loss 5.4043\n",
      "E4/25 | batch 00850 | loss 5.5236\n",
      "E4/25 | batch 00900 | loss 5.6255\n",
      "E4/25 | batch 00950 | loss 5.4615\n",
      "E4/25 | batch 01000 | loss 5.6862\n",
      "E4/25 | batch 01050 | loss 5.3971\n",
      "E4/25 | batch 01100 | loss 5.2890\n",
      "E4/25 | batch 01150 | loss 5.3718\n",
      "E4/25 | batch 01200 | loss 5.4346\n",
      "E4/25 | batch 01250 | loss 5.5642\n",
      "E4/25 | batch 01300 | loss 5.4482\n",
      "E4/25 | batch 01350 | loss 5.4760\n",
      "E4/25 | batch 01400 | loss 5.4069\n",
      "E4/25 | batch 01450 | loss 5.5892\n",
      "E4/25 | batch 01500 | loss 5.4103\n",
      "E4/25 | batch 01550 | loss 5.5632\n",
      "E4/25 | batch 01600 | loss 5.4257\n",
      "E4/25 | batch 01650 | loss 5.6154\n",
      "E4/25 | batch 01700 | loss 5.6256\n",
      "E4/25 | batch 01750 | loss 5.5665\n",
      "Epoch 4/25 done in 314.9s | train=5.4972 | val=5.5599 ✅ best\n",
      "E5/25 | batch 00001 | loss 5.4967\n",
      "E5/25 | batch 00050 | loss 5.3848\n",
      "E5/25 | batch 00100 | loss 5.4329\n",
      "E5/25 | batch 00150 | loss 5.3992\n",
      "E5/25 | batch 00200 | loss 5.3464\n",
      "E5/25 | batch 00250 | loss 5.3583\n",
      "E5/25 | batch 00300 | loss 5.4812\n",
      "E5/25 | batch 00350 | loss 5.3221\n",
      "E5/25 | batch 00400 | loss 5.4300\n",
      "E5/25 | batch 00450 | loss 5.2984\n",
      "E5/25 | batch 00500 | loss 5.3886\n",
      "E5/25 | batch 00550 | loss 5.2550\n",
      "E5/25 | batch 00600 | loss 5.3520\n",
      "E5/25 | batch 00650 | loss 5.3385\n",
      "E5/25 | batch 00700 | loss 5.3331\n",
      "E5/25 | batch 00750 | loss 5.2633\n",
      "E5/25 | batch 00800 | loss 5.3841\n",
      "E5/25 | batch 00850 | loss 5.4461\n",
      "E5/25 | batch 00900 | loss 5.3175\n",
      "E5/25 | batch 00950 | loss 5.4449\n",
      "E5/25 | batch 01000 | loss 5.3402\n",
      "E5/25 | batch 01050 | loss 5.4135\n",
      "E5/25 | batch 01100 | loss 5.3359\n",
      "E5/25 | batch 01150 | loss 5.4372\n",
      "E5/25 | batch 01200 | loss 5.4756\n",
      "E5/25 | batch 01250 | loss 5.2920\n",
      "E5/25 | batch 01300 | loss 5.5333\n",
      "E5/25 | batch 01350 | loss 5.3992\n",
      "E5/25 | batch 01400 | loss 5.4459\n",
      "E5/25 | batch 01450 | loss 5.6384\n",
      "E5/25 | batch 01500 | loss 5.4306\n",
      "E5/25 | batch 01550 | loss 5.2465\n",
      "E5/25 | batch 01600 | loss 5.3434\n",
      "E5/25 | batch 01650 | loss 5.2677\n",
      "E5/25 | batch 01700 | loss 5.4439\n",
      "E5/25 | batch 01750 | loss 5.2530\n",
      "Epoch 5/25 done in 312.2s | train=5.3910 | val=5.5185 ✅ best\n",
      "E6/25 | batch 00001 | loss 5.4454\n",
      "E6/25 | batch 00050 | loss 5.2744\n",
      "E6/25 | batch 00100 | loss 5.2644\n",
      "E6/25 | batch 00150 | loss 5.2754\n",
      "E6/25 | batch 00200 | loss 5.5535\n",
      "E6/25 | batch 00250 | loss 5.3038\n",
      "E6/25 | batch 00300 | loss 5.2740\n",
      "E6/25 | batch 00350 | loss 5.2872\n",
      "E6/25 | batch 00400 | loss 5.1648\n",
      "E6/25 | batch 00450 | loss 5.2323\n",
      "E6/25 | batch 00500 | loss 5.2965\n",
      "E6/25 | batch 00550 | loss 5.3657\n",
      "E6/25 | batch 00600 | loss 5.3466\n",
      "E6/25 | batch 00650 | loss 5.2831\n",
      "E6/25 | batch 00700 | loss 5.4137\n",
      "E6/25 | batch 00750 | loss 5.2482\n",
      "E6/25 | batch 00800 | loss 5.3074\n",
      "E6/25 | batch 00850 | loss 5.3525\n",
      "E6/25 | batch 00900 | loss 5.1892\n",
      "E6/25 | batch 00950 | loss 5.2838\n",
      "E6/25 | batch 01000 | loss 5.1401\n",
      "E6/25 | batch 01050 | loss 5.2737\n",
      "E6/25 | batch 01100 | loss 5.3888\n",
      "E6/25 | batch 01150 | loss 5.2038\n",
      "E6/25 | batch 01200 | loss 5.2627\n",
      "E6/25 | batch 01250 | loss 5.3632\n",
      "E6/25 | batch 01300 | loss 5.1809\n",
      "E6/25 | batch 01350 | loss 5.3338\n",
      "E6/25 | batch 01400 | loss 5.3759\n",
      "E6/25 | batch 01450 | loss 5.2919\n",
      "E6/25 | batch 01500 | loss 5.1876\n",
      "E6/25 | batch 01550 | loss 5.2484\n",
      "E6/25 | batch 01600 | loss 5.2905\n",
      "E6/25 | batch 01650 | loss 5.4547\n",
      "E6/25 | batch 01700 | loss 5.5655\n",
      "E6/25 | batch 01750 | loss 5.3983\n",
      "Epoch 6/25 done in 299.8s | train=5.2986 | val=5.4969 ✅ best\n",
      "E7/25 | batch 00001 | loss 5.2675\n",
      "E7/25 | batch 00050 | loss 5.0656\n",
      "E7/25 | batch 00100 | loss 5.2060\n",
      "E7/25 | batch 00150 | loss 5.4814\n",
      "E7/25 | batch 00200 | loss 5.2334\n",
      "E7/25 | batch 00250 | loss 5.3183\n",
      "E7/25 | batch 00300 | loss 5.1645\n",
      "E7/25 | batch 00350 | loss 5.1488\n",
      "E7/25 | batch 00400 | loss 5.3097\n",
      "E7/25 | batch 00450 | loss 5.3293\n",
      "E7/25 | batch 00500 | loss 5.3193\n",
      "E7/25 | batch 00550 | loss 5.0337\n",
      "E7/25 | batch 00600 | loss 5.2024\n",
      "E7/25 | batch 00650 | loss 5.3979\n",
      "E7/25 | batch 00700 | loss 5.1616\n",
      "E7/25 | batch 00750 | loss 5.3113\n",
      "E7/25 | batch 00800 | loss 5.2009\n",
      "E7/25 | batch 00850 | loss 5.1110\n",
      "E7/25 | batch 00900 | loss 5.1167\n",
      "E7/25 | batch 00950 | loss 5.1578\n",
      "E7/25 | batch 01000 | loss 4.9799\n",
      "E7/25 | batch 01050 | loss 5.2408\n",
      "E7/25 | batch 01100 | loss 5.2496\n",
      "E7/25 | batch 01150 | loss 5.1760\n",
      "E7/25 | batch 01200 | loss 5.2690\n",
      "E7/25 | batch 01250 | loss 5.3342\n",
      "E7/25 | batch 01300 | loss 5.2904\n",
      "E7/25 | batch 01350 | loss 5.2253\n",
      "E7/25 | batch 01400 | loss 5.2825\n",
      "E7/25 | batch 01450 | loss 5.2734\n",
      "E7/25 | batch 01500 | loss 5.2494\n",
      "E7/25 | batch 01550 | loss 5.3317\n",
      "E7/25 | batch 01600 | loss 5.3293\n",
      "E7/25 | batch 01650 | loss 5.2204\n",
      "E7/25 | batch 01700 | loss 5.2037\n",
      "E7/25 | batch 01750 | loss 5.1635\n",
      "Epoch 7/25 done in 301.6s | train=5.2133 | val=5.4850 ✅ best\n",
      "E8/25 | batch 00001 | loss 5.2515\n",
      "E8/25 | batch 00050 | loss 4.9928\n",
      "E8/25 | batch 00100 | loss 5.0865\n",
      "E8/25 | batch 00150 | loss 5.0163\n",
      "E8/25 | batch 00200 | loss 5.3608\n",
      "E8/25 | batch 00250 | loss 5.2163\n",
      "E8/25 | batch 00300 | loss 5.1878\n",
      "E8/25 | batch 00350 | loss 5.1072\n",
      "E8/25 | batch 00400 | loss 5.0949\n",
      "E8/25 | batch 00450 | loss 5.1227\n",
      "E8/25 | batch 00500 | loss 5.1298\n",
      "E8/25 | batch 00550 | loss 5.1644\n",
      "E8/25 | batch 00600 | loss 5.0992\n",
      "E8/25 | batch 00650 | loss 5.1885\n",
      "E8/25 | batch 00700 | loss 5.1238\n",
      "E8/25 | batch 00750 | loss 4.9350\n",
      "E8/25 | batch 00800 | loss 5.4257\n",
      "E8/25 | batch 00850 | loss 5.1674\n",
      "E8/25 | batch 00900 | loss 5.2093\n",
      "E8/25 | batch 00950 | loss 5.2739\n",
      "E8/25 | batch 01000 | loss 4.8229\n",
      "E8/25 | batch 01050 | loss 5.1140\n",
      "E8/25 | batch 01100 | loss 5.0873\n",
      "E8/25 | batch 01150 | loss 5.2190\n",
      "E8/25 | batch 01200 | loss 5.1080\n",
      "E8/25 | batch 01250 | loss 4.9315\n",
      "E8/25 | batch 01300 | loss 5.1943\n",
      "E8/25 | batch 01350 | loss 5.2622\n",
      "E8/25 | batch 01400 | loss 5.1902\n",
      "E8/25 | batch 01450 | loss 5.2634\n",
      "E8/25 | batch 01500 | loss 5.1640\n",
      "E8/25 | batch 01550 | loss 4.9305\n",
      "E8/25 | batch 01600 | loss 5.1871\n",
      "E8/25 | batch 01650 | loss 4.9110\n",
      "E8/25 | batch 01700 | loss 5.3369\n",
      "E8/25 | batch 01750 | loss 5.0992\n",
      "Epoch 8/25 done in 308.4s | train=5.1332 | val=5.4605 ✅ best\n",
      "E9/25 | batch 00001 | loss 5.0211\n",
      "E9/25 | batch 00050 | loss 5.0185\n",
      "E9/25 | batch 00100 | loss 5.0363\n",
      "E9/25 | batch 00150 | loss 5.0387\n",
      "E9/25 | batch 00200 | loss 5.1116\n",
      "E9/25 | batch 00250 | loss 5.0415\n",
      "E9/25 | batch 00300 | loss 5.0701\n",
      "E9/25 | batch 00350 | loss 5.0831\n",
      "E9/25 | batch 00400 | loss 4.9394\n",
      "E9/25 | batch 00450 | loss 5.0114\n",
      "E9/25 | batch 00500 | loss 4.9600\n",
      "E9/25 | batch 00550 | loss 4.8847\n",
      "E9/25 | batch 00600 | loss 4.8879\n",
      "E9/25 | batch 00650 | loss 5.0067\n",
      "E9/25 | batch 00700 | loss 5.2228\n",
      "E9/25 | batch 00750 | loss 5.1524\n",
      "E9/25 | batch 00800 | loss 5.1945\n",
      "E9/25 | batch 00850 | loss 5.0403\n",
      "E9/25 | batch 00900 | loss 5.0591\n",
      "E9/25 | batch 00950 | loss 5.0851\n",
      "E9/25 | batch 01000 | loss 4.9455\n",
      "E9/25 | batch 01050 | loss 4.8619\n",
      "E9/25 | batch 01100 | loss 4.9746\n",
      "E9/25 | batch 01150 | loss 4.9445\n",
      "E9/25 | batch 01200 | loss 5.1036\n",
      "E9/25 | batch 01250 | loss 5.0009\n",
      "E9/25 | batch 01300 | loss 5.1003\n",
      "E9/25 | batch 01350 | loss 4.9198\n",
      "E9/25 | batch 01400 | loss 5.0123\n",
      "E9/25 | batch 01450 | loss 5.1511\n",
      "E9/25 | batch 01500 | loss 5.1963\n",
      "E9/25 | batch 01550 | loss 5.0253\n",
      "E9/25 | batch 01600 | loss 5.0011\n",
      "E9/25 | batch 01650 | loss 5.1544\n",
      "E9/25 | batch 01700 | loss 5.2482\n",
      "E9/25 | batch 01750 | loss 5.1919\n",
      "Epoch 9/25 done in 301.6s | train=5.0552 | val=5.4726 \n",
      "E10/25 | batch 00001 | loss 5.0173\n",
      "E10/25 | batch 00050 | loss 4.7830\n",
      "E10/25 | batch 00100 | loss 4.9079\n",
      "E10/25 | batch 00150 | loss 4.9822\n",
      "E10/25 | batch 00200 | loss 4.9449\n",
      "E10/25 | batch 00250 | loss 4.8989\n",
      "E10/25 | batch 00300 | loss 4.8511\n",
      "E10/25 | batch 00350 | loss 4.8747\n",
      "E10/25 | batch 00400 | loss 4.8407\n",
      "E10/25 | batch 00450 | loss 4.8534\n",
      "E10/25 | batch 00500 | loss 5.0617\n",
      "E10/25 | batch 00550 | loss 5.0404\n",
      "E10/25 | batch 00600 | loss 5.0263\n",
      "E10/25 | batch 00650 | loss 5.1715\n",
      "E10/25 | batch 00700 | loss 4.9886\n",
      "E10/25 | batch 00750 | loss 4.9072\n",
      "E10/25 | batch 00800 | loss 5.0228\n",
      "E10/25 | batch 00850 | loss 5.0601\n",
      "E10/25 | batch 00900 | loss 4.9279\n",
      "E10/25 | batch 00950 | loss 5.1197\n",
      "E10/25 | batch 01000 | loss 5.0140\n",
      "E10/25 | batch 01050 | loss 5.0669\n",
      "E10/25 | batch 01100 | loss 4.7277\n",
      "E10/25 | batch 01150 | loss 5.1168\n",
      "E10/25 | batch 01200 | loss 4.8652\n",
      "E10/25 | batch 01250 | loss 4.9557\n",
      "E10/25 | batch 01300 | loss 4.7407\n",
      "E10/25 | batch 01350 | loss 5.0651\n",
      "E10/25 | batch 01400 | loss 4.9235\n",
      "E10/25 | batch 01450 | loss 5.0186\n",
      "E10/25 | batch 01500 | loss 5.0432\n",
      "E10/25 | batch 01550 | loss 5.0226\n",
      "E10/25 | batch 01600 | loss 4.9472\n",
      "E10/25 | batch 01650 | loss 4.9208\n",
      "E10/25 | batch 01700 | loss 4.9700\n",
      "E10/25 | batch 01750 | loss 5.0555\n",
      "Epoch 10/25 done in 299.6s | train=4.9757 | val=5.4310 ✅ best\n",
      "E11/25 | batch 00001 | loss 4.9685\n",
      "E11/25 | batch 00050 | loss 4.8656\n",
      "E11/25 | batch 00100 | loss 4.7979\n",
      "E11/25 | batch 00150 | loss 4.9194\n",
      "E11/25 | batch 00200 | loss 5.0193\n",
      "E11/25 | batch 00250 | loss 4.8313\n",
      "E11/25 | batch 00300 | loss 4.7836\n",
      "E11/25 | batch 00350 | loss 4.7810\n",
      "E11/25 | batch 00400 | loss 5.0548\n",
      "E11/25 | batch 00450 | loss 4.9166\n",
      "E11/25 | batch 00500 | loss 4.9160\n",
      "E11/25 | batch 00550 | loss 4.8023\n",
      "E11/25 | batch 00600 | loss 4.8341\n",
      "E11/25 | batch 00650 | loss 4.8314\n",
      "E11/25 | batch 00700 | loss 5.0987\n",
      "E11/25 | batch 00750 | loss 4.7992\n",
      "E11/25 | batch 00800 | loss 4.8738\n",
      "E11/25 | batch 00850 | loss 4.9851\n",
      "E11/25 | batch 00900 | loss 4.8713\n",
      "E11/25 | batch 00950 | loss 4.8177\n",
      "E11/25 | batch 01000 | loss 4.9185\n",
      "E11/25 | batch 01050 | loss 4.8007\n",
      "E11/25 | batch 01100 | loss 4.8593\n",
      "E11/25 | batch 01150 | loss 4.8647\n",
      "E11/25 | batch 01200 | loss 5.0360\n",
      "E11/25 | batch 01250 | loss 4.8542\n",
      "E11/25 | batch 01300 | loss 4.8980\n",
      "E11/25 | batch 01350 | loss 4.9489\n",
      "E11/25 | batch 01400 | loss 4.9059\n",
      "E11/25 | batch 01450 | loss 4.8819\n",
      "E11/25 | batch 01500 | loss 4.8383\n",
      "E11/25 | batch 01550 | loss 4.9081\n",
      "E11/25 | batch 01600 | loss 4.9423\n",
      "E11/25 | batch 01650 | loss 4.9045\n",
      "E11/25 | batch 01700 | loss 4.7353\n",
      "E11/25 | batch 01750 | loss 5.0766\n",
      "Epoch 11/25 done in 295.3s | train=4.8928 | val=5.4084 ✅ best\n",
      "E12/25 | batch 00001 | loss 4.7655\n",
      "E12/25 | batch 00050 | loss 4.7721\n",
      "E12/25 | batch 00100 | loss 4.6185\n",
      "E12/25 | batch 00150 | loss 4.7497\n",
      "E12/25 | batch 00200 | loss 4.8283\n",
      "E12/25 | batch 00250 | loss 4.7339\n",
      "E12/25 | batch 00300 | loss 4.7182\n",
      "E12/25 | batch 00350 | loss 4.7849\n",
      "E12/25 | batch 00400 | loss 4.6659\n",
      "E12/25 | batch 00450 | loss 4.9178\n",
      "E12/25 | batch 00500 | loss 4.7931\n",
      "E12/25 | batch 00550 | loss 4.6732\n",
      "E12/25 | batch 00600 | loss 4.7563\n",
      "E12/25 | batch 00650 | loss 4.7936\n",
      "E12/25 | batch 00700 | loss 4.7623\n",
      "E12/25 | batch 00750 | loss 4.8390\n",
      "E12/25 | batch 00800 | loss 4.8865\n",
      "E12/25 | batch 00850 | loss 4.7012\n",
      "E12/25 | batch 00900 | loss 5.0226\n",
      "E12/25 | batch 00950 | loss 4.9034\n",
      "E12/25 | batch 01000 | loss 5.0529\n",
      "E12/25 | batch 01050 | loss 4.7496\n",
      "E12/25 | batch 01100 | loss 4.8348\n",
      "E12/25 | batch 01150 | loss 4.7554\n",
      "E12/25 | batch 01200 | loss 4.8457\n",
      "E12/25 | batch 01250 | loss 4.9298\n",
      "E12/25 | batch 01300 | loss 4.6927\n",
      "E12/25 | batch 01350 | loss 4.8754\n",
      "E12/25 | batch 01400 | loss 4.7546\n",
      "E12/25 | batch 01450 | loss 4.7639\n",
      "E12/25 | batch 01500 | loss 4.7830\n",
      "E12/25 | batch 01550 | loss 4.7304\n",
      "E12/25 | batch 01600 | loss 4.7175\n",
      "E12/25 | batch 01650 | loss 4.9411\n",
      "E12/25 | batch 01700 | loss 4.6987\n",
      "E12/25 | batch 01750 | loss 4.8334\n",
      "Epoch 12/25 done in 293.5s | train=4.8044 | val=5.3664 ✅ best\n",
      "E13/25 | batch 00001 | loss 4.6837\n",
      "E13/25 | batch 00050 | loss 4.7496\n",
      "E13/25 | batch 00100 | loss 4.7116\n",
      "E13/25 | batch 00150 | loss 4.5658\n",
      "E13/25 | batch 00200 | loss 4.7363\n",
      "E13/25 | batch 00250 | loss 4.7206\n",
      "E13/25 | batch 00300 | loss 4.7301\n",
      "E13/25 | batch 00350 | loss 4.6754\n",
      "E13/25 | batch 00400 | loss 4.6753\n",
      "E13/25 | batch 00450 | loss 4.8019\n",
      "E13/25 | batch 00500 | loss 4.8502\n",
      "E13/25 | batch 00550 | loss 4.6155\n",
      "E13/25 | batch 00600 | loss 4.7660\n",
      "E13/25 | batch 00650 | loss 4.7453\n",
      "E13/25 | batch 00700 | loss 4.7264\n",
      "E13/25 | batch 00750 | loss 4.6974\n",
      "E13/25 | batch 00800 | loss 4.6849\n",
      "E13/25 | batch 00850 | loss 4.9109\n",
      "E13/25 | batch 00900 | loss 4.8495\n",
      "E13/25 | batch 00950 | loss 4.7364\n",
      "E13/25 | batch 01000 | loss 4.7034\n",
      "E13/25 | batch 01050 | loss 4.7795\n",
      "E13/25 | batch 01100 | loss 4.6950\n",
      "E13/25 | batch 01150 | loss 4.8413\n",
      "E13/25 | batch 01200 | loss 4.8149\n",
      "E13/25 | batch 01250 | loss 4.6228\n",
      "E13/25 | batch 01300 | loss 4.8388\n",
      "E13/25 | batch 01350 | loss 4.9064\n",
      "E13/25 | batch 01400 | loss 4.7474\n",
      "E13/25 | batch 01450 | loss 4.6217\n",
      "E13/25 | batch 01500 | loss 4.8590\n",
      "E13/25 | batch 01550 | loss 4.7097\n",
      "E13/25 | batch 01600 | loss 4.8073\n",
      "E13/25 | batch 01650 | loss 4.6374\n",
      "E13/25 | batch 01700 | loss 4.8277\n",
      "E13/25 | batch 01750 | loss 4.7038\n",
      "Epoch 13/25 done in 296.3s | train=4.7034 | val=5.2836 ✅ best\n",
      "E14/25 | batch 00001 | loss 4.4852\n",
      "E14/25 | batch 00050 | loss 4.4701\n",
      "E14/25 | batch 00100 | loss 4.4243\n",
      "E14/25 | batch 00150 | loss 4.4938\n",
      "E14/25 | batch 00200 | loss 4.4853\n",
      "E14/25 | batch 00250 | loss 4.4736\n",
      "E14/25 | batch 00300 | loss 4.6811\n",
      "E14/25 | batch 00350 | loss 4.5381\n",
      "E14/25 | batch 00400 | loss 4.4387\n",
      "E14/25 | batch 00450 | loss 4.7621\n",
      "E14/25 | batch 00500 | loss 4.4224\n",
      "E14/25 | batch 00550 | loss 4.7038\n",
      "E14/25 | batch 00600 | loss 4.3273\n",
      "E14/25 | batch 00650 | loss 4.4942\n",
      "E14/25 | batch 00700 | loss 4.4472\n",
      "E14/25 | batch 00750 | loss 4.5224\n",
      "E14/25 | batch 00800 | loss 4.5633\n",
      "E14/25 | batch 00850 | loss 4.5981\n",
      "E14/25 | batch 00900 | loss 4.4865\n",
      "E14/25 | batch 00950 | loss 4.8814\n",
      "E14/25 | batch 01000 | loss 4.6726\n",
      "E14/25 | batch 01050 | loss 4.5203\n",
      "E14/25 | batch 01100 | loss 4.6806\n",
      "E14/25 | batch 01150 | loss 4.7479\n",
      "E14/25 | batch 01200 | loss 4.3993\n",
      "E14/25 | batch 01250 | loss 4.6290\n",
      "E14/25 | batch 01300 | loss 4.5015\n",
      "E14/25 | batch 01350 | loss 4.7064\n",
      "E14/25 | batch 01400 | loss 4.7408\n",
      "E14/25 | batch 01450 | loss 4.5944\n",
      "E14/25 | batch 01500 | loss 4.4492\n",
      "E14/25 | batch 01550 | loss 4.5885\n",
      "E14/25 | batch 01600 | loss 4.3639\n",
      "E14/25 | batch 01650 | loss 4.7795\n",
      "E14/25 | batch 01700 | loss 4.6015\n",
      "E14/25 | batch 01750 | loss 4.6541\n",
      "Epoch 14/25 done in 295.4s | train=4.5828 | val=5.1532 ✅ best\n",
      "E15/25 | batch 00001 | loss 4.3378\n",
      "E15/25 | batch 00050 | loss 4.3592\n",
      "E15/25 | batch 00100 | loss 4.1445\n",
      "E15/25 | batch 00150 | loss 4.3523\n",
      "E15/25 | batch 00200 | loss 4.3664\n",
      "E15/25 | batch 00250 | loss 4.3415\n",
      "E15/25 | batch 00300 | loss 4.3067\n",
      "E15/25 | batch 00350 | loss 4.2694\n",
      "E15/25 | batch 00400 | loss 4.5057\n",
      "E15/25 | batch 00450 | loss 4.5228\n",
      "E15/25 | batch 00500 | loss 4.4703\n",
      "E15/25 | batch 00550 | loss 4.4316\n",
      "E15/25 | batch 00600 | loss 4.5125\n",
      "E15/25 | batch 00650 | loss 4.6544\n",
      "E15/25 | batch 00700 | loss 4.5566\n",
      "E15/25 | batch 00750 | loss 4.4625\n",
      "E15/25 | batch 00800 | loss 4.4188\n",
      "E15/25 | batch 00850 | loss 4.3461\n",
      "E15/25 | batch 00900 | loss 4.5606\n",
      "E15/25 | batch 00950 | loss 4.5294\n",
      "E15/25 | batch 01000 | loss 4.5263\n",
      "E15/25 | batch 01050 | loss 4.4008\n",
      "E15/25 | batch 01100 | loss 4.6120\n",
      "E15/25 | batch 01150 | loss 4.4699\n",
      "E15/25 | batch 01200 | loss 4.3678\n",
      "E15/25 | batch 01250 | loss 4.5684\n",
      "E15/25 | batch 01300 | loss 4.4426\n",
      "E15/25 | batch 01350 | loss 4.7734\n",
      "E15/25 | batch 01400 | loss 4.4197\n",
      "E15/25 | batch 01450 | loss 4.4141\n",
      "E15/25 | batch 01500 | loss 4.4730\n",
      "E15/25 | batch 01550 | loss 4.4848\n",
      "E15/25 | batch 01600 | loss 4.4577\n",
      "E15/25 | batch 01650 | loss 4.4020\n",
      "E15/25 | batch 01700 | loss 4.4091\n",
      "E15/25 | batch 01750 | loss 4.5354\n",
      "Epoch 15/25 done in 293.7s | train=4.4387 | val=5.0599 ✅ best\n",
      "E16/25 | batch 00001 | loss 4.2598\n",
      "E16/25 | batch 00050 | loss 4.1620\n",
      "E16/25 | batch 00100 | loss 4.1305\n",
      "E16/25 | batch 00150 | loss 4.1045\n",
      "E16/25 | batch 00200 | loss 4.3755\n",
      "E16/25 | batch 00250 | loss 4.3809\n",
      "E16/25 | batch 00300 | loss 4.2456\n",
      "E16/25 | batch 00350 | loss 4.2452\n",
      "E16/25 | batch 00400 | loss 4.1633\n",
      "E16/25 | batch 00450 | loss 4.2557\n",
      "E16/25 | batch 00500 | loss 4.2943\n",
      "E16/25 | batch 00550 | loss 4.2085\n",
      "E16/25 | batch 00600 | loss 4.4071\n",
      "E16/25 | batch 00650 | loss 4.2405\n",
      "E16/25 | batch 00700 | loss 4.4092\n",
      "E16/25 | batch 00750 | loss 4.3016\n",
      "E16/25 | batch 00800 | loss 4.1820\n",
      "E16/25 | batch 00850 | loss 4.3565\n",
      "E16/25 | batch 00900 | loss 4.2023\n",
      "E16/25 | batch 00950 | loss 4.2538\n",
      "E16/25 | batch 01000 | loss 4.1984\n",
      "E16/25 | batch 01050 | loss 4.1339\n",
      "E16/25 | batch 01100 | loss 4.2806\n",
      "E16/25 | batch 01150 | loss 4.1479\n",
      "E16/25 | batch 01200 | loss 4.3194\n",
      "E16/25 | batch 01250 | loss 4.3339\n",
      "E16/25 | batch 01300 | loss 4.2999\n",
      "E16/25 | batch 01350 | loss 4.3038\n",
      "E16/25 | batch 01400 | loss 4.1263\n",
      "E16/25 | batch 01450 | loss 4.2520\n",
      "E16/25 | batch 01500 | loss 4.2942\n",
      "E16/25 | batch 01550 | loss 4.2401\n",
      "E16/25 | batch 01600 | loss 4.2777\n",
      "E16/25 | batch 01650 | loss 4.4594\n",
      "E16/25 | batch 01700 | loss 4.4089\n",
      "E16/25 | batch 01750 | loss 4.2928\n",
      "Epoch 16/25 done in 290.1s | train=4.2801 | val=4.8433 ✅ best\n",
      "E17/25 | batch 00001 | loss 4.1413\n",
      "E17/25 | batch 00050 | loss 4.0780\n",
      "E17/25 | batch 00100 | loss 4.0701\n",
      "E17/25 | batch 00150 | loss 4.0999\n",
      "E17/25 | batch 00200 | loss 4.0635\n",
      "E17/25 | batch 00250 | loss 4.1493\n",
      "E17/25 | batch 00300 | loss 4.0472\n",
      "E17/25 | batch 00350 | loss 4.1698\n",
      "E17/25 | batch 00400 | loss 3.9308\n",
      "E17/25 | batch 00450 | loss 4.1685\n",
      "E17/25 | batch 00500 | loss 4.0702\n",
      "E17/25 | batch 00550 | loss 4.1062\n",
      "E17/25 | batch 00600 | loss 4.1232\n",
      "E17/25 | batch 00650 | loss 4.1035\n",
      "E17/25 | batch 00700 | loss 4.0731\n",
      "E17/25 | batch 00750 | loss 4.2709\n",
      "E17/25 | batch 00800 | loss 4.1364\n",
      "E17/25 | batch 00850 | loss 4.1027\n",
      "E17/25 | batch 00900 | loss 4.1107\n",
      "E17/25 | batch 00950 | loss 4.2193\n",
      "E17/25 | batch 01000 | loss 4.1937\n",
      "E17/25 | batch 01050 | loss 4.0535\n",
      "E17/25 | batch 01100 | loss 4.1012\n",
      "E17/25 | batch 01150 | loss 4.1103\n",
      "E17/25 | batch 01200 | loss 4.0462\n",
      "E17/25 | batch 01250 | loss 4.0295\n",
      "E17/25 | batch 01300 | loss 4.0892\n",
      "E17/25 | batch 01350 | loss 4.0878\n",
      "E17/25 | batch 01400 | loss 4.2844\n",
      "E17/25 | batch 01450 | loss 4.0785\n",
      "E17/25 | batch 01500 | loss 4.3008\n",
      "E17/25 | batch 01550 | loss 4.2727\n",
      "E17/25 | batch 01600 | loss 4.3559\n",
      "E17/25 | batch 01650 | loss 4.1228\n",
      "E17/25 | batch 01700 | loss 4.1774\n",
      "E17/25 | batch 01750 | loss 3.9788\n",
      "Epoch 17/25 done in 292.4s | train=4.1086 | val=4.6254 ✅ best\n",
      "E18/25 | batch 00001 | loss 3.7631\n",
      "E18/25 | batch 00050 | loss 4.0002\n",
      "E18/25 | batch 00100 | loss 3.9397\n",
      "E18/25 | batch 00150 | loss 3.9590\n",
      "E18/25 | batch 00200 | loss 3.8130\n",
      "E18/25 | batch 00250 | loss 3.9493\n",
      "E18/25 | batch 00300 | loss 3.7555\n",
      "E18/25 | batch 00350 | loss 3.9130\n",
      "E18/25 | batch 00400 | loss 3.9818\n",
      "E18/25 | batch 00450 | loss 3.8961\n",
      "E18/25 | batch 00500 | loss 3.9706\n",
      "E18/25 | batch 00550 | loss 3.9034\n",
      "E18/25 | batch 00600 | loss 4.0221\n",
      "E18/25 | batch 00650 | loss 3.8313\n",
      "E18/25 | batch 00700 | loss 3.9985\n",
      "E18/25 | batch 00750 | loss 3.8475\n",
      "E18/25 | batch 00800 | loss 3.9623\n",
      "E18/25 | batch 00850 | loss 3.9036\n",
      "E18/25 | batch 00900 | loss 3.8943\n",
      "E18/25 | batch 00950 | loss 3.8012\n",
      "E18/25 | batch 01000 | loss 3.9272\n",
      "E18/25 | batch 01050 | loss 3.8214\n",
      "E18/25 | batch 01100 | loss 3.8824\n",
      "E18/25 | batch 01150 | loss 4.1839\n",
      "E18/25 | batch 01200 | loss 4.0503\n",
      "E18/25 | batch 01250 | loss 3.8401\n",
      "E18/25 | batch 01300 | loss 4.0483\n",
      "E18/25 | batch 01350 | loss 3.8614\n",
      "E18/25 | batch 01400 | loss 3.8882\n",
      "E18/25 | batch 01450 | loss 3.8840\n",
      "E18/25 | batch 01500 | loss 4.1346\n",
      "E18/25 | batch 01550 | loss 3.9530\n",
      "E18/25 | batch 01600 | loss 3.9677\n",
      "E18/25 | batch 01650 | loss 4.3496\n",
      "E18/25 | batch 01700 | loss 3.9474\n",
      "E18/25 | batch 01750 | loss 4.1470\n",
      "Epoch 18/25 done in 322.5s | train=3.9379 | val=4.4841 ✅ best\n",
      "E19/25 | batch 00001 | loss 3.8014\n",
      "E19/25 | batch 00050 | loss 3.5572\n",
      "E19/25 | batch 00100 | loss 3.6941\n",
      "E19/25 | batch 00150 | loss 3.7157\n",
      "E19/25 | batch 00200 | loss 3.8434\n",
      "E19/25 | batch 00250 | loss 3.6479\n",
      "E19/25 | batch 00300 | loss 3.8617\n",
      "E19/25 | batch 00350 | loss 3.7528\n",
      "E19/25 | batch 00400 | loss 3.8686\n",
      "E19/25 | batch 00450 | loss 3.6140\n",
      "E19/25 | batch 00500 | loss 3.6309\n",
      "E19/25 | batch 00550 | loss 3.6753\n",
      "E19/25 | batch 00600 | loss 3.4912\n",
      "E19/25 | batch 00650 | loss 3.6958\n",
      "E19/25 | batch 00700 | loss 3.6837\n",
      "E19/25 | batch 00750 | loss 3.8367\n",
      "E19/25 | batch 00800 | loss 3.7698\n",
      "E19/25 | batch 00850 | loss 3.8607\n",
      "E19/25 | batch 00900 | loss 3.7501\n",
      "E19/25 | batch 00950 | loss 3.6795\n",
      "E19/25 | batch 01000 | loss 3.8236\n",
      "E19/25 | batch 01050 | loss 3.7478\n",
      "E19/25 | batch 01100 | loss 3.7665\n",
      "E19/25 | batch 01150 | loss 3.6723\n",
      "E19/25 | batch 01200 | loss 3.7702\n",
      "E19/25 | batch 01250 | loss 3.7433\n",
      "E19/25 | batch 01300 | loss 3.7144\n",
      "E19/25 | batch 01350 | loss 3.8586\n",
      "E19/25 | batch 01400 | loss 3.7544\n",
      "E19/25 | batch 01450 | loss 3.6908\n",
      "E19/25 | batch 01500 | loss 3.8647\n",
      "E19/25 | batch 01550 | loss 3.8817\n",
      "E19/25 | batch 01600 | loss 3.7498\n",
      "E19/25 | batch 01650 | loss 3.6365\n",
      "E19/25 | batch 01700 | loss 3.7457\n",
      "E19/25 | batch 01750 | loss 3.7289\n",
      "Epoch 19/25 done in 312.3s | train=3.7396 | val=4.1845 ✅ best\n",
      "E20/25 | batch 00001 | loss 3.5795\n",
      "E20/25 | batch 00050 | loss 3.3039\n",
      "E20/25 | batch 00100 | loss 3.4566\n",
      "E20/25 | batch 00150 | loss 3.5833\n",
      "E20/25 | batch 00200 | loss 3.4423\n",
      "E20/25 | batch 00250 | loss 3.6284\n",
      "E20/25 | batch 00300 | loss 3.5408\n",
      "E20/25 | batch 00350 | loss 3.5215\n",
      "E20/25 | batch 00400 | loss 3.5639\n",
      "E20/25 | batch 00450 | loss 3.4510\n",
      "E20/25 | batch 00500 | loss 3.6465\n",
      "E20/25 | batch 00550 | loss 3.3440\n",
      "E20/25 | batch 00600 | loss 3.6533\n",
      "E20/25 | batch 00650 | loss 3.4043\n",
      "E20/25 | batch 00700 | loss 3.5222\n",
      "E20/25 | batch 00750 | loss 3.6558\n",
      "E20/25 | batch 00800 | loss 3.6504\n",
      "E20/25 | batch 00850 | loss 3.5319\n",
      "E20/25 | batch 00900 | loss 3.5711\n",
      "E20/25 | batch 00950 | loss 3.3676\n",
      "E20/25 | batch 01000 | loss 3.4837\n",
      "E20/25 | batch 01050 | loss 3.5437\n",
      "E20/25 | batch 01100 | loss 3.4137\n",
      "E20/25 | batch 01150 | loss 3.5638\n",
      "E20/25 | batch 01200 | loss 3.6334\n",
      "E20/25 | batch 01250 | loss 3.6679\n",
      "E20/25 | batch 01300 | loss 3.5698\n",
      "E20/25 | batch 01350 | loss 3.6541\n",
      "E20/25 | batch 01400 | loss 3.4920\n",
      "E20/25 | batch 01450 | loss 3.6471\n",
      "E20/25 | batch 01500 | loss 3.4541\n",
      "E20/25 | batch 01550 | loss 3.4767\n",
      "E20/25 | batch 01600 | loss 3.6347\n",
      "E20/25 | batch 01650 | loss 3.6366\n",
      "E20/25 | batch 01700 | loss 3.5142\n",
      "E20/25 | batch 01750 | loss 3.4725\n",
      "Epoch 20/25 done in 289.8s | train=3.5334 | val=3.9605 ✅ best\n",
      "E21/25 | batch 00001 | loss 3.3871\n",
      "E21/25 | batch 00050 | loss 3.3852\n",
      "E21/25 | batch 00100 | loss 3.4533\n",
      "E21/25 | batch 00150 | loss 3.1503\n",
      "E21/25 | batch 00200 | loss 3.2213\n",
      "E21/25 | batch 00250 | loss 2.8762\n",
      "E21/25 | batch 00300 | loss 3.2845\n",
      "E21/25 | batch 00350 | loss 3.2919\n",
      "E21/25 | batch 00400 | loss 3.1378\n",
      "E21/25 | batch 00450 | loss 3.3603\n",
      "E21/25 | batch 00500 | loss 3.3173\n",
      "E21/25 | batch 00550 | loss 3.0163\n",
      "E21/25 | batch 00600 | loss 3.6479\n",
      "E21/25 | batch 00650 | loss 3.4747\n",
      "E21/25 | batch 00700 | loss 3.2755\n",
      "E21/25 | batch 00750 | loss 3.3511\n",
      "E21/25 | batch 00800 | loss 3.0588\n",
      "E21/25 | batch 00850 | loss 3.4575\n",
      "E21/25 | batch 00900 | loss 3.4573\n",
      "E21/25 | batch 00950 | loss 3.1693\n",
      "E21/25 | batch 01000 | loss 3.3319\n",
      "E21/25 | batch 01050 | loss 3.4266\n",
      "E21/25 | batch 01100 | loss 3.2365\n",
      "E21/25 | batch 01150 | loss 3.4026\n",
      "E21/25 | batch 01200 | loss 3.2890\n",
      "E21/25 | batch 01250 | loss 3.5478\n",
      "E21/25 | batch 01300 | loss 3.3581\n",
      "E21/25 | batch 01350 | loss 3.2120\n",
      "E21/25 | batch 01400 | loss 3.2827\n",
      "E21/25 | batch 01450 | loss 3.2230\n",
      "E21/25 | batch 01500 | loss 3.2552\n",
      "E21/25 | batch 01550 | loss 3.2982\n",
      "E21/25 | batch 01600 | loss 3.3317\n",
      "E21/25 | batch 01650 | loss 3.3693\n",
      "E21/25 | batch 01700 | loss 3.2874\n",
      "E21/25 | batch 01750 | loss 3.1969\n",
      "Epoch 21/25 done in 285.0s | train=3.3167 | val=3.6374 ✅ best\n",
      "E22/25 | batch 00001 | loss 3.0910\n",
      "E22/25 | batch 00050 | loss 3.2463\n",
      "E22/25 | batch 00100 | loss 2.9497\n",
      "E22/25 | batch 00150 | loss 3.0649\n",
      "E22/25 | batch 00200 | loss 3.2593\n",
      "E22/25 | batch 00250 | loss 3.2110\n",
      "E22/25 | batch 00300 | loss 3.3950\n",
      "E22/25 | batch 00350 | loss 3.0969\n",
      "E22/25 | batch 00400 | loss 3.2567\n",
      "E22/25 | batch 00450 | loss 3.0271\n",
      "E22/25 | batch 00500 | loss 3.1262\n",
      "E22/25 | batch 00550 | loss 2.9736\n",
      "E22/25 | batch 00600 | loss 3.2171\n",
      "E22/25 | batch 00650 | loss 3.1380\n",
      "E22/25 | batch 00700 | loss 2.9773\n",
      "E22/25 | batch 00750 | loss 2.9700\n",
      "E22/25 | batch 00800 | loss 2.9845\n",
      "E22/25 | batch 00850 | loss 3.1751\n",
      "E22/25 | batch 00900 | loss 3.1528\n",
      "E22/25 | batch 00950 | loss 3.2378\n",
      "E22/25 | batch 01000 | loss 3.0424\n",
      "E22/25 | batch 01050 | loss 2.9982\n",
      "E22/25 | batch 01100 | loss 3.0874\n",
      "E22/25 | batch 01150 | loss 2.9564\n",
      "E22/25 | batch 01200 | loss 2.9968\n",
      "E22/25 | batch 01250 | loss 3.1685\n",
      "E22/25 | batch 01300 | loss 3.3466\n",
      "E22/25 | batch 01350 | loss 2.8018\n",
      "E22/25 | batch 01400 | loss 2.9942\n",
      "E22/25 | batch 01450 | loss 2.9484\n",
      "E22/25 | batch 01500 | loss 2.9321\n",
      "E22/25 | batch 01550 | loss 3.0949\n",
      "E22/25 | batch 01600 | loss 3.0894\n",
      "E22/25 | batch 01650 | loss 3.1272\n",
      "E22/25 | batch 01700 | loss 3.2427\n",
      "E22/25 | batch 01750 | loss 2.9743\n",
      "Epoch 22/25 done in 379.8s | train=3.0756 | val=3.4054 ✅ best\n",
      "E23/25 | batch 00001 | loss 2.8817\n",
      "E23/25 | batch 00050 | loss 2.8757\n",
      "E23/25 | batch 00100 | loss 2.6556\n",
      "E23/25 | batch 00150 | loss 2.7965\n",
      "E23/25 | batch 00200 | loss 2.6926\n",
      "E23/25 | batch 00250 | loss 2.7067\n",
      "E23/25 | batch 00300 | loss 2.7822\n",
      "E23/25 | batch 00350 | loss 3.0345\n",
      "E23/25 | batch 00400 | loss 2.8329\n",
      "E23/25 | batch 00450 | loss 2.6943\n",
      "E23/25 | batch 00500 | loss 2.8134\n",
      "E23/25 | batch 00550 | loss 2.8875\n",
      "E23/25 | batch 00600 | loss 2.7536\n",
      "E23/25 | batch 00650 | loss 2.7336\n",
      "E23/25 | batch 00700 | loss 3.1256\n",
      "E23/25 | batch 00750 | loss 2.8813\n",
      "E23/25 | batch 00800 | loss 2.7976\n",
      "E23/25 | batch 00850 | loss 3.1427\n",
      "E23/25 | batch 00900 | loss 2.9138\n",
      "E23/25 | batch 00950 | loss 2.7154\n",
      "E23/25 | batch 01000 | loss 2.7313\n",
      "E23/25 | batch 01050 | loss 2.8486\n",
      "E23/25 | batch 01100 | loss 2.7468\n",
      "E23/25 | batch 01150 | loss 2.9831\n",
      "E23/25 | batch 01200 | loss 2.6574\n",
      "E23/25 | batch 01250 | loss 2.9464\n",
      "E23/25 | batch 01300 | loss 2.7638\n",
      "E23/25 | batch 01350 | loss 2.9131\n",
      "E23/25 | batch 01400 | loss 2.9720\n",
      "E23/25 | batch 01450 | loss 2.7411\n",
      "E23/25 | batch 01500 | loss 2.9769\n",
      "E23/25 | batch 01550 | loss 2.6966\n",
      "E23/25 | batch 01600 | loss 2.8308\n",
      "E23/25 | batch 01650 | loss 2.8428\n",
      "E23/25 | batch 01700 | loss 2.7458\n",
      "E23/25 | batch 01750 | loss 2.8278\n",
      "Epoch 23/25 done in 416.2s | train=2.8398 | val=3.1372 ✅ best\n",
      "E24/25 | batch 00001 | loss 2.5855\n",
      "E24/25 | batch 00050 | loss 2.4854\n",
      "E24/25 | batch 00100 | loss 2.8486\n",
      "E24/25 | batch 00150 | loss 2.5968\n",
      "E24/25 | batch 00200 | loss 2.4830\n",
      "E24/25 | batch 00250 | loss 2.7569\n",
      "E24/25 | batch 00300 | loss 2.3049\n",
      "E24/25 | batch 00350 | loss 2.6101\n",
      "E24/25 | batch 00400 | loss 2.5930\n",
      "E24/25 | batch 00450 | loss 2.5470\n",
      "E24/25 | batch 00500 | loss 2.6062\n",
      "E24/25 | batch 00550 | loss 2.6235\n",
      "E24/25 | batch 00600 | loss 2.6587\n",
      "E24/25 | batch 00650 | loss 2.4053\n",
      "E24/25 | batch 00700 | loss 2.7564\n",
      "E24/25 | batch 00750 | loss 2.5060\n",
      "E24/25 | batch 00800 | loss 2.7814\n",
      "E24/25 | batch 00850 | loss 2.6489\n",
      "E24/25 | batch 00900 | loss 2.4543\n",
      "E24/25 | batch 00950 | loss 2.7007\n",
      "E24/25 | batch 01000 | loss 2.3974\n",
      "E24/25 | batch 01050 | loss 2.6463\n",
      "E24/25 | batch 01100 | loss 2.5189\n",
      "E24/25 | batch 01150 | loss 2.5350\n",
      "E24/25 | batch 01200 | loss 2.7246\n",
      "E24/25 | batch 01250 | loss 2.5249\n",
      "E24/25 | batch 01300 | loss 2.5743\n",
      "E24/25 | batch 01350 | loss 2.4203\n",
      "E24/25 | batch 01400 | loss 2.8167\n",
      "E24/25 | batch 01450 | loss 2.6778\n",
      "E24/25 | batch 01500 | loss 2.8951\n",
      "E24/25 | batch 01550 | loss 2.8229\n",
      "E24/25 | batch 01600 | loss 2.6833\n",
      "E24/25 | batch 01650 | loss 2.4766\n",
      "E24/25 | batch 01700 | loss 2.6733\n",
      "E24/25 | batch 01750 | loss 2.6847\n",
      "Epoch 24/25 done in 302.7s | train=2.6127 | val=2.8917 ✅ best\n",
      "E25/25 | batch 00001 | loss 2.4065\n",
      "E25/25 | batch 00050 | loss 2.1493\n",
      "E25/25 | batch 00100 | loss 2.4127\n",
      "E25/25 | batch 00150 | loss 2.4683\n",
      "E25/25 | batch 00200 | loss 2.5662\n",
      "E25/25 | batch 00250 | loss 2.3125\n",
      "E25/25 | batch 00300 | loss 2.4559\n",
      "E25/25 | batch 00350 | loss 2.4596\n",
      "E25/25 | batch 00400 | loss 2.5078\n",
      "E25/25 | batch 00450 | loss 2.3196\n",
      "E25/25 | batch 00500 | loss 2.4432\n",
      "E25/25 | batch 00550 | loss 2.4842\n",
      "E25/25 | batch 00600 | loss 2.4294\n",
      "E25/25 | batch 00650 | loss 2.6009\n",
      "E25/25 | batch 00700 | loss 2.5075\n",
      "E25/25 | batch 00750 | loss 2.3413\n",
      "E25/25 | batch 00800 | loss 2.5122\n",
      "E25/25 | batch 00850 | loss 2.3434\n",
      "E25/25 | batch 00900 | loss 2.3588\n",
      "E25/25 | batch 00950 | loss 2.6606\n",
      "E25/25 | batch 01000 | loss 2.9224\n",
      "E25/25 | batch 01050 | loss 2.6071\n",
      "E25/25 | batch 01100 | loss 2.4027\n",
      "E25/25 | batch 01150 | loss 2.3764\n",
      "E25/25 | batch 01200 | loss 2.6119\n",
      "E25/25 | batch 01250 | loss 2.5140\n",
      "E25/25 | batch 01300 | loss 2.3465\n",
      "E25/25 | batch 01350 | loss 2.5530\n",
      "E25/25 | batch 01400 | loss 2.3014\n",
      "E25/25 | batch 01450 | loss 2.4775\n",
      "E25/25 | batch 01500 | loss 2.4510\n",
      "E25/25 | batch 01550 | loss 2.3354\n",
      "E25/25 | batch 01600 | loss 2.3681\n",
      "E25/25 | batch 01650 | loss 2.3152\n",
      "E25/25 | batch 01700 | loss 2.4302\n",
      "E25/25 | batch 01750 | loss 2.3190\n",
      "Epoch 25/25 done in 299.2s | train=2.4224 | val=2.7379 ✅ best\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(state.epoch, EPOCHS):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    total, steps = 0.0, 0\n",
    "\n",
    "    for b, (feats, toks, feat_lens, tok_lens) in enumerate(train_loader):\n",
    "        feats, toks = feats.to(device), toks.to(device)\n",
    "        feat_lens, tok_lens = feat_lens.to(device), tok_lens.to(device)\n",
    "\n",
    "        if USE_SPEC_AUG:\n",
    "            specaug.train()\n",
    "            feats = specaug(feats)\n",
    "\n",
    "        tgt_in  = toks[:, :-1]\n",
    "        tgt_out = toks[:, 1:]\n",
    "        tgt_pad_mask = make_key_padding_mask(tok_lens - 1, tgt_in.size(1))\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", enabled=use_cuda):\n",
    "            logits = model(feats, tgt_in, src_lengths=feat_lens, tgt_key_padding_mask=tgt_pad_mask)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total += float(loss.item())\n",
    "        steps += 1\n",
    "\n",
    "        if (b == 0) or ((b + 1) % LOG_EVERY == 0):\n",
    "            print(f\"E{epoch+1}/{EPOCHS} | batch {b+1:05d} | loss {loss.item():.4f}\")\n",
    "\n",
    "    train_loss = total / max(1, steps)\n",
    "    val_loss = run_eval_loss(val_loader)\n",
    "\n",
    "    # Save last\n",
    "    torch.save({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"best_val\": state.best_val,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optim\": optimizer.state_dict(),\n",
    "        \"sp_model\": SP_MODEL,\n",
    "    }, LAST_CKPT)\n",
    "\n",
    "    # Save best\n",
    "    best_tag = \"\"\n",
    "    if val_loss < state.best_val:\n",
    "        state.best_val = val_loss\n",
    "        torch.save({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"best_val\": state.best_val,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optim\": optimizer.state_dict(),\n",
    "            \"sp_model\": SP_MODEL,\n",
    "        }, BEST_CKPT)\n",
    "        best_tag = \"✅ best\"\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} done in {time.time()-t0:.1f}s | train={train_loss:.4f} | val={val_loss:.4f} {best_tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5xdBAWq2TU01"
   },
   "outputs": [],
   "source": [
    "def norm_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "@torch.inference_mode()\n",
    "def beam_search_decode(model, feats, feat_len, beam_size=5, max_len=200, len_penalty=0.6):\n",
    "    model.eval()\n",
    "    device_ = feats.device\n",
    "\n",
    "    beams = [(torch.tensor([[BOS_ID]], device=device_, dtype=torch.long), 0.0, False)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for ys, score, finished in beams:\n",
    "            if finished:\n",
    "                new_beams.append((ys, score, True))\n",
    "                continue\n",
    "\n",
    "            logits = model(feats, ys, src_lengths=feat_len, tgt_key_padding_mask=None)\n",
    "            log_probs = torch.log_softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "            topk = torch.topk(log_probs, k=beam_size, dim=-1)\n",
    "            vals = topk.values.squeeze(0)\n",
    "            idxs = topk.indices.squeeze(0)\n",
    "\n",
    "            for lp, tid in zip(vals.tolist(), idxs.tolist()):\n",
    "                ys2 = torch.cat([ys, torch.tensor([[tid]], device=device_, dtype=torch.long)], dim=1)\n",
    "                new_beams.append((ys2, score + lp, tid == EOS_ID))\n",
    "\n",
    "        def rank_key(item):\n",
    "            ys, sc, fin = item\n",
    "            L = ys.size(1)\n",
    "            lp = ((5 + L) / 6) ** len_penalty\n",
    "            return sc / lp\n",
    "\n",
    "        new_beams.sort(key=rank_key, reverse=True)\n",
    "        beams = new_beams[:beam_size]\n",
    "\n",
    "        if all(fin for _, _, fin in beams):\n",
    "            break\n",
    "\n",
    "    best = max(beams, key=lambda x: x[1])[0].squeeze(0).tolist()\n",
    "    return best\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_wer_cer_fast(loader, max_utterances=30, beam_size=5, max_len=200):\n",
    "    model.eval()\n",
    "    refs, hyps = [], []\n",
    "    n = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for feats, toks, feat_lens, tok_lens in loader:\n",
    "        feats, toks = feats.to(device), toks.to(device)\n",
    "        feat_lens = feat_lens.to(device)\n",
    "\n",
    "        B = feats.size(0)\n",
    "        for i in range(B):\n",
    "            f = feats[i:i+1]\n",
    "            fl = feat_lens[i:i+1]\n",
    "\n",
    "            pred_ids = beam_search_decode(model, f, fl, beam_size=beam_size, max_len=max_len)\n",
    "            if pred_ids and pred_ids[0] == BOS_ID:\n",
    "                pred_ids = pred_ids[1:]\n",
    "            if EOS_ID in pred_ids:\n",
    "                pred_ids = pred_ids[:pred_ids.index(EOS_ID)]\n",
    "            pred_ids = [x for x in pred_ids if x != PAD_ID]\n",
    "            pred_txt = sp.decode(pred_ids)\n",
    "\n",
    "            ref_ids = [x for x in toks[i].tolist() if x not in (PAD_ID, BOS_ID, EOS_ID)]\n",
    "            ref_txt = sp.decode(ref_ids)\n",
    "\n",
    "            refs.append(norm_text(ref_txt))\n",
    "            hyps.append(norm_text(pred_txt))\n",
    "            n += 1\n",
    "\n",
    "            if n % 10 == 0:\n",
    "                print(f\"Decoded {n}/{max_utterances} | WER~{wer(refs, hyps):.3f} CER~{cer(refs, hyps):.3f} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "            if n >= max_utterances:\n",
    "                print(f\"Eval | WER={wer(refs, hyps):.4f} | CER={cer(refs, hyps):.4f} | N={n}\")\n",
    "                return\n",
    "\n",
    "    print(f\"Eval | WER={wer(refs, hyps):.4f} | CER={cer(refs, hyps):.4f} | N={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "J2E6T9IbTXCQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint: ./checkpoints_en\\asr_en_best.pt\n",
      "Decoded 10/200 | WER~0.583 CER~0.371 | 21.3s\n",
      "Decoded 20/200 | WER~0.500 CER~0.334 | 47.7s\n",
      "Decoded 30/200 | WER~0.512 CER~0.335 | 77.3s\n",
      "Decoded 40/200 | WER~0.497 CER~0.329 | 91.4s\n",
      "Decoded 50/200 | WER~0.493 CER~0.330 | 112.0s\n",
      "Decoded 60/200 | WER~0.483 CER~0.324 | 135.4s\n",
      "Decoded 70/200 | WER~0.504 CER~0.339 | 161.7s\n",
      "Decoded 80/200 | WER~0.504 CER~0.345 | 185.2s\n",
      "Decoded 90/200 | WER~0.494 CER~0.338 | 218.4s\n",
      "Decoded 100/200 | WER~0.502 CER~0.344 | 236.1s\n",
      "Decoded 110/200 | WER~0.517 CER~0.352 | 258.2s\n",
      "Decoded 120/200 | WER~0.524 CER~0.355 | 271.1s\n",
      "Decoded 130/200 | WER~0.545 CER~0.367 | 307.8s\n",
      "Decoded 140/200 | WER~0.547 CER~0.368 | 327.6s\n",
      "Decoded 150/200 | WER~0.546 CER~0.368 | 340.8s\n",
      "Decoded 160/200 | WER~0.546 CER~0.368 | 364.6s\n",
      "Decoded 170/200 | WER~0.540 CER~0.364 | 386.6s\n",
      "Decoded 180/200 | WER~0.535 CER~0.361 | 418.1s\n",
      "Decoded 190/200 | WER~0.536 CER~0.365 | 453.5s\n",
      "Decoded 200/200 | WER~0.534 CER~0.365 | 491.1s\n",
      "Eval | WER=0.5338 | CER=0.3655 | N=200\n",
      "\n",
      "Audio: ./datasets\\LibriSpeech\\LibriSpeech\\test-clean\\1995\\1826\\1995-1826-0017.flac\n",
      "REF : THERE MIGHT BE A BIT OF POETRY HERE AND THERE BUT MOST OF THIS PLACE WAS SUCH DESPERATE PROSE\n",
      "HYP : there might be a good voyage here in that most of his place was such doubtfuls\n",
      "\n",
      "REF(norm): there might be a bit of poetry here and there but most of this place was such desperate prose\n",
      "HYP(norm): there might be a good voyage here in that most of his place was such doubtfuls\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Load BEST checkpoint (safe)\n",
    "# ----------------------------\n",
    "if os.path.exists(BEST_CKPT):\n",
    "    ckpt = torch.load(BEST_CKPT, map_location=device, weights_only=True)\n",
    "    # If your checkpoint is a dict like {\"model\": state_dict, ...}\n",
    "    if isinstance(ckpt, dict) and \"model\" in ckpt:\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "    else:\n",
    "        # fallback if you saved the raw state_dict\n",
    "        model.load_state_dict(ckpt)\n",
    "    print(\"Loaded best checkpoint:\", BEST_CKPT)\n",
    "else:\n",
    "    print(\"BEST checkpoint not found:\", BEST_CKPT)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate WER/CER (more stable)\n",
    "# ----------------------------\n",
    "EVAL_UTT = 200        # 30 is noisy; 200 is much more reliable\n",
    "BEAM = 8\n",
    "MAX_LEN = 220\n",
    "LEN_PEN = 0.8\n",
    "\n",
    "# If your eval_wer_cer_fast doesn't take len_penalty, keep it as-is.\n",
    "# Otherwise, update it similarly (optional).\n",
    "eval_wer_cer_fast(test_loader, max_utterances=EVAL_UTT, beam_size=BEAM, max_len=MAX_LEN)\n",
    "\n",
    "# ----------------------------\n",
    "# Show one random sample\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "\n",
    "sample_path, sample_text = random.choice(test_pairs)\n",
    "wav, _ = load_audio_16k_mono(sample_path)\n",
    "\n",
    "# Use same feature extraction as training\n",
    "mel_fn = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=16000, n_fft=1024, hop_length=256, n_mels=80\n",
    ")\n",
    "mel = mel_fn(wav.unsqueeze(0))\n",
    "feat = torch.log(mel + 1e-9).squeeze(0).T  # (T, 80)\n",
    "\n",
    "feats = feat.unsqueeze(0).to(device)\n",
    "feat_len = torch.tensor([feat.size(0)], device=device, dtype=torch.long)\n",
    "\n",
    "# Decode with beam search (stronger settings)\n",
    "pred_ids = beam_search_decode(\n",
    "    model, feats, feat_len,\n",
    "    beam_size=BEAM, max_len=250, len_penalty=LEN_PEN\n",
    ")\n",
    "\n",
    "# Strip BOS/EOS/PAD\n",
    "if pred_ids and pred_ids[0] == BOS_ID:\n",
    "    pred_ids = pred_ids[1:]\n",
    "if EOS_ID in pred_ids:\n",
    "    pred_ids = pred_ids[:pred_ids.index(EOS_ID)]\n",
    "pred_ids = [x for x in pred_ids if x != PAD_ID]\n",
    "\n",
    "pred_text = sp.decode(pred_ids)\n",
    "\n",
    "print(\"\\nAudio:\", sample_path)\n",
    "print(\"REF :\", sample_text)\n",
    "print(\"HYP :\", pred_text)\n",
    "\n",
    "# Optional: also show normalized versions (useful for debugging WER differences)\n",
    "print(\"\\nREF(norm):\", norm_text(sample_text))\n",
    "print(\"HYP(norm):\", norm_text(pred_text))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPrb+56L540c1aoC5py4Sb",
   "provenance": [
    {
     "file_id": "1mgKblYJ3H8Jt9m9sJ41LdF0QL4lr0oZU",
     "timestamp": 1767971770933
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
